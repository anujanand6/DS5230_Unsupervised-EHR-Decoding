{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import xmltodict\n",
    "\n",
    "import spacy\n",
    "import scispacy\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Base English model\n",
    "from spacy.lang.en import English\n",
    "en = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ScispaCy model\n",
    "# To install, please do:\n",
    "# pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_md-0.3.0.tar.gz\n",
    "\n",
    "import en_core_sci_md\n",
    "scispacy = en_core_sci_md.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_ehr(path):\n",
    "    with open(path, 'r') as file:\n",
    "        doc=file.read()\n",
    "    doc_main = xmltodict.parse(doc)['PatientMatching']\n",
    "    doc_txt = doc_main['TEXT']\n",
    "    doc_tags = doc_main['TAGS']\n",
    "    return doc_txt, doc_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tags_dict(xml_tags):\n",
    "    tag_dict = {}\n",
    "    for name in xml_tags:\n",
    "        tag_dict[name] = xml_tags[name][\"@met\"]\n",
    "    return tag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_txt(xml_txt):\n",
    "    new_txt=re.sub(\"\\\\n\", \" \", xml_txt)\n",
    "    new_txt = re.sub(\"[\\s\\W]{4,}|\\_+\", \" \", new_txt)\n",
    "    return new_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_records={}\n",
    "\n",
    "directory = 'train/'\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.xml'):\n",
    "        file_id = int(filename.split('.')[0])\n",
    "        xml_txt, xml_tags = open_ehr(os.path.join(directory, filename))\n",
    "        tags_dict = create_tags_dict(xml_tags)\n",
    "        text = clean_txt(xml_txt)\n",
    "        train_records[file_id] = {'tags':tags_dict,\n",
    "                                   'text':text}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Tokenizer\n",
    "\n",
    "def adv_tokenizer(doc, model=en, \n",
    "                  replace_entities=False, \n",
    "                  remove_stopwords=True, \n",
    "                  lowercase=True, \n",
    "                  alpha_only=True, \n",
    "                  lemma=True):\n",
    "    \"\"\"Full tokenizer with flags for processing steps\n",
    "    replace_entities: If True, replaces with entity type\n",
    "    stop_words: If False, removes stop words\n",
    "    lowercase: If True, lowercases all tokens\n",
    "    alpha_only: If True, removes all non-alpha characters\n",
    "    lemma: If True, lemmatizes words\n",
    "    \"\"\"\n",
    "    parsed = model(doc)\n",
    "    # token collector\n",
    "    tokens = []\n",
    "    # index pointer\n",
    "    i = 0\n",
    "    # entity collector\n",
    "    ent = ''\n",
    "    for t in parsed:\n",
    "        # only need this if we're replacing entities\n",
    "        if replace_entities:\n",
    "            # replace URLs\n",
    "            if t.like_url:\n",
    "                tokens.append('URL')\n",
    "                continue\n",
    "            # if there's entities collected and current token is non-entity\n",
    "            if (t.ent_iob_=='O')&(ent!=''):\n",
    "                tokens.append(ent)\n",
    "                ent = ''\n",
    "                continue\n",
    "            elif t.ent_iob_!='O':\n",
    "                ent = t.ent_type_\n",
    "                continue\n",
    "        # only include stop words if stop words==True\n",
    "        if (t.is_stop) & (remove_stopwords):\n",
    "            continue\n",
    "        # only include non-alpha is alpha_only==False\n",
    "        if (not t.is_alpha)&(alpha_only):\n",
    "            continue\n",
    "        if lemma:\n",
    "            t = t.lemma_\n",
    "        else:\n",
    "            t = t.text\n",
    "        if lowercase:\n",
    "            t.lower() \n",
    "        tokens.append(t)   \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return the top N tokens using Count Vectorizer\n",
    "def top_N_words(text, N=10, model=en, replace_entities=False, remove_stopwords=True, lowercase=True, \n",
    "                alpha_only=True, lemma=True, min_df=1, max_df=1.0, ngram=1):\n",
    "    \n",
    "  # running this on negative reviews\n",
    "  cv = CountVectorizer(tokenizer=lambda text: adv_tokenizer(text, model=model, \n",
    "                                                            lemma=lemma, \n",
    "                                                            replace_entities=replace_entities,\n",
    "                                                            lowercase=lowercase, \n",
    "                                                            remove_stopwords=remove_stopwords,\n",
    "                                                            alpha_only=alpha_only), \n",
    "                       min_df=min_df, max_df=max_df, ngram_range=(ngram,ngram))\n",
    "  \n",
    "  cv_vectors = cv.fit_transform(text).toarray()\n",
    "  # get_feature_names gets the vocabulary of the vectorizer in order\n",
    "  word_count = dict(zip(cv.get_feature_names(), cv_vectors.sum(axis=0)))\n",
    "  # get the top N words\n",
    "  return sorted(word_count.items(), key=lambda x: x[1], reverse=True)[:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top N words present:\n",
      "[('weight', 11), ('chest', 9), ('russell', 9), ('clear', 6), ('mercy', 6), ('pressure', 6), ('donna', 5), ('follow', 5), ('lose', 5), ('assessment', 4)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top N words in one document\n",
    "text = train_records[101]['text']\n",
    "\n",
    "print(\"Top N words present:\")\n",
    "print(top_N_words([text], N=10, ngram=1), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top N words present:\n",
      "[('mg', 6047), ('po', 3019), ('patient', 2707), ('history', 2553), ('qd', 2228), ('pain', 2139), ('date', 1591), ('chest', 1507), ('l', 1478), ('normal', 1439)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top N words in a list of documents\n",
    "text_list = []\n",
    "for record in train_records.values():\n",
    "    text_list.append(record['text'])\n",
    "\n",
    "print(\"Top N words present:\")\n",
    "print(top_N_words(text_list, N=10, ngram=1), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top N words present:\n",
      "[('URL', 66)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top N words in a list of documents\n",
    "text_list = []\n",
    "for record in train_records.values():\n",
    "    text_list.append(record['text'])\n",
    "\n",
    "print(\"Top N words present:\")\n",
    "print(top_N_words(text_list, N=10, ngram=1, replace_entities=True), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
