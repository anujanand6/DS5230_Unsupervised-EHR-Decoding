{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import xmltodict\n",
    "\n",
    "import spacy\n",
    "import scispacy\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Base English model\n",
    "from spacy.lang.en import English\n",
    "en = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ScispaCy model\n",
    "# To install, please do:\n",
    "# pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_md-0.3.0.tar.gz\n",
    "\n",
    "import en_core_sci_md\n",
    "scispacy = en_core_sci_md.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_ehr(path):\n",
    "    with open(path, 'r') as file:\n",
    "        doc=file.read()\n",
    "    doc_main = xmltodict.parse(doc)['PatientMatching']\n",
    "    doc_txt = doc_main['TEXT']\n",
    "    doc_tags = doc_main['TAGS']\n",
    "    return doc_txt, doc_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tags_dict(xml_tags):\n",
    "    tag_dict = {}\n",
    "    for name in xml_tags:\n",
    "        tag_dict[name] = xml_tags[name][\"@met\"]\n",
    "    return tag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_txt(xml_txt):\n",
    "    new_txt=re.sub(\"\\\\n\", \" \", xml_txt)\n",
    "    new_txt = re.sub(\"[\\s\\W]{4,}|\\_+\", \" \", new_txt)\n",
    "    return new_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_records={}\n",
    "\n",
    "directory = '../train/'\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.xml'):\n",
    "        file_id = int(filename.split('.')[0])\n",
    "        xml_txt, xml_tags = open_ehr(os.path.join(directory, filename))\n",
    "        tags_dict = create_tags_dict(xml_tags)\n",
    "        text = clean_txt(xml_txt)\n",
    "        train_records[file_id] = {'tags':tags_dict,\n",
    "                                   'text':text}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Tokenizer\n",
    "\n",
    "def adv_tokenizer(doc, model=en, \n",
    "                  replace_entities=False, \n",
    "                  remove_stopwords=True, \n",
    "                  lowercase=True, \n",
    "                  alpha_only=True, \n",
    "                  lemma=True):\n",
    "    \"\"\"Full tokenizer with flags for processing steps\n",
    "    replace_entities: If True, replaces with entity type\n",
    "    stop_words: If False, removes stop words\n",
    "    lowercase: If True, lowercases all tokens\n",
    "    alpha_only: If True, removes all non-alpha characters\n",
    "    lemma: If True, lemmatizes words\n",
    "    \"\"\"\n",
    "    parsed = model(doc)\n",
    "    # token collector\n",
    "    tokens = []\n",
    "    # index pointer\n",
    "    i = 0\n",
    "    # entity collector\n",
    "    ent = ''\n",
    "    for t in parsed:\n",
    "        # only need this if we're replacing entities\n",
    "        if replace_entities:\n",
    "            # replace URLs\n",
    "            if t.like_url:\n",
    "                tokens.append('URL')\n",
    "                continue\n",
    "            # if there's entities collected and current token is non-entity\n",
    "            if (t.ent_iob_=='O')&(ent!=''):\n",
    "                tokens.append(ent)\n",
    "                ent = ''\n",
    "                continue\n",
    "            elif t.ent_iob_!='O':\n",
    "                ent = t.ent_type_\n",
    "                continue\n",
    "        # only include stop words if stop words==True\n",
    "        if (t.is_stop) & (remove_stopwords):\n",
    "            continue\n",
    "        # only include non-alpha is alpha_only==False\n",
    "        if (not t.is_alpha)&(alpha_only):\n",
    "            continue\n",
    "        if lemma:\n",
    "            t = t.lemma_\n",
    "        else:\n",
    "            t = t.text\n",
    "        if lowercase:\n",
    "            t.lower() \n",
    "        tokens.append(t)   \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return the top N tokens using Count Vectorizer\n",
    "def top_N_words(text, N=10, model=en, replace_entities=False, remove_stopwords=True, lowercase=True, \n",
    "                alpha_only=True, lemma=True, min_df=1, max_df=1.0, ngram=1):\n",
    "    \n",
    "  # running this on negative reviews\n",
    "  cv = CountVectorizer(tokenizer=lambda text: adv_tokenizer(text, model=model, \n",
    "                                                            lemma=lemma, \n",
    "                                                            replace_entities=replace_entities,\n",
    "                                                            lowercase=lowercase, \n",
    "                                                            remove_stopwords=remove_stopwords,\n",
    "                                                            alpha_only=alpha_only), \n",
    "                       min_df=min_df, max_df=max_df, ngram_range=(ngram,ngram))\n",
    "  \n",
    "  cv_vectors = cv.fit_transform(text).toarray()\n",
    "  # get_feature_names gets the vocabulary of the vectorizer in order\n",
    "  word_count = dict(zip(cv.get_feature_names(), cv_vectors.sum(axis=0)))\n",
    "  # get the top N words\n",
    "  return sorted(word_count.items(), key=lambda x: x[1], reverse=True)[:N]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Token Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top N words present:\n",
      "[('weight', 11), ('chest', 9), ('russell', 9), ('clear', 6), ('mercy', 6), ('pressure', 6), ('donna', 5), ('follow', 5), ('lose', 5), ('assessment', 4)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top N words in one document\n",
    "text = train_records[101]['text']\n",
    "\n",
    "print(\"Top N words present:\")\n",
    "print(top_N_words([text], N=10, ngram=1), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top N words present:\n",
      "[('mg', 6047), ('po', 3019), ('patient', 2707), ('history', 2553), ('qd', 2228), ('pain', 2139), ('date', 1591), ('chest', 1507), ('l', 1478), ('normal', 1439)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top N words in a list of documents\n",
    "text_list = []\n",
    "for record in train_records.values():\n",
    "    text_list.append(record['text'])\n",
    "\n",
    "print(\"Top N words present:\")\n",
    "print(top_N_words(text_list, N=10, ngram=1), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top N words present:\n",
      "[('URL', 66)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top N words in a list of documents\n",
    "text_list = []\n",
    "for record in train_records.values():\n",
    "    text_list.append(record['text'])\n",
    "\n",
    "print(\"Top N words present:\")\n",
    "print(top_N_words(text_list, N=10, ngram=1, replace_entities=True), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Top Tokens for Conditions Met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_by_tag = {}\n",
    "for tag in train_records[162]['tags']:\n",
    "    text_by_tag[tag] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary where key = tag; value = array of texts that meet tag criteria\n",
    "for record in train_records.values():\n",
    "    for tag in text_by_tag:\n",
    "        if record['tags'][tag] == 'met':\n",
    "            text_by_tag[tag].append(record['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in text with criteria for ABDOMINAL:\n",
      "[('mg', 5306), ('po', 2758), ('history', 2202), ('patient', 2154), ('qd', 2048), ('pain', 1914), ('l', 1480), ('date', 1450), ('normal', 1260), ('left', 1152)] \n",
      "\n",
      "Top 10 words in text with criteria for ADVANCED-CAD:\n",
      "[('mg', 7522), ('po', 3534), ('patient', 3234), ('history', 3186), ('pain', 2910), ('qd', 2872), ('chest', 2082), ('date', 1874), ('normal', 1794), ('l', 1758)] \n",
      "\n",
      "Top 10 words in text with criteria for ALCOHOL-ABUSE:\n",
      "[('mg', 294), ('history', 224), ('pain', 186), ('patient', 186), ('po', 166), ('l', 148), ('normal', 146), ('dr', 112), ('qd', 112), ('blood', 94)] \n",
      "\n",
      "Top 10 words in text with criteria for ASP-FOR-MI:\n",
      "[('mg', 10306), ('po', 5048), ('patient', 4262), ('history', 4106), ('qd', 3910), ('pain', 3634), ('chest', 2548), ('date', 2490), ('l', 2422), ('normal', 2358)] \n",
      "\n",
      "Top 10 words in text with criteria for CREATININE:\n",
      "[('mg', 4756), ('patient', 2422), ('po', 2406), ('history', 2220), ('qd', 1690), ('pain', 1528), ('date', 1420), ('l', 1372), ('left', 1222), ('chest', 1182)] \n",
      "\n",
      "Top 10 words in text with criteria for DIETSUPP-2MOS:\n",
      "[('mg', 7676), ('po', 4182), ('qd', 2888), ('patient', 2752), ('history', 2562), ('pain', 2240), ('l', 2160), ('date', 1748), ('normal', 1582), ('chest', 1532)] \n",
      "\n",
      "Top 10 words in text with criteria for DRUG-ABUSE:\n",
      "[('mg', 838), ('po', 532), ('history', 350), ('qd', 328), ('patient', 312), ('pain', 274), ('pt', 210), ('normal', 194), ('chest', 192), ('bid', 186)] \n",
      "\n",
      "Top 10 words in text with criteria for ENGLISH:\n",
      "[('mg', 11330), ('po', 5614), ('patient', 4986), ('history', 4880), ('qd', 4184), ('pain', 3906), ('date', 3066), ('l', 2852), ('chest', 2830), ('normal', 2732)] \n",
      "\n",
      "Top 10 words in text with criteria for HBA1C:\n",
      "[('mg', 4770), ('po', 2484), ('qd', 1740), ('patient', 1678), ('history', 1638), ('pain', 1518), ('date', 1022), ('chest', 1004), ('normal', 976), ('l', 974)] \n",
      "\n",
      "Top 10 words in text with criteria for KETO-1YR:\n",
      "[('mg', 102), ('l', 84), ('h', 66), ('o', 64), ('pain', 62), ('history', 60), ('po', 50), ('cord', 46), ('spine', 46), ('date', 42)] \n",
      "\n",
      "Top 10 words in text with criteria for MAJOR-DIABETES:\n",
      "[('mg', 7158), ('po', 3724), ('patient', 3258), ('history', 2974), ('qd', 2696), ('pain', 2304), ('date', 1936), ('l', 1918), ('left', 1722), ('dr', 1574)] \n",
      "\n",
      "Top 10 words in text with criteria for MAKES-DECISIONS:\n",
      "[('mg', 11648), ('po', 5794), ('patient', 5026), ('history', 4904), ('qd', 4282), ('pain', 4190), ('date', 3060), ('chest', 2940), ('l', 2826), ('normal', 2746)] \n",
      "\n",
      "Top 10 words in text with criteria for MI-6MOS:\n",
      "[('mg', 1076), ('po', 546), ('pain', 516), ('patient', 454), ('qd', 440), ('l', 416), ('history', 368), ('pt', 318), ('chest', 286), ('date', 268)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_tokens = {}\n",
    "for tag in text_by_tag:\n",
    "    top_tokens[tag] = top_N_words(text_by_tag[tag], N=10, ngram=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "##  add frequent document words to stop words\n",
    "##  histograms for all conditions\n",
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
